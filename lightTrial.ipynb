{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/49/b95c037b717b4ceadc76b6e164603471225c27052d1611d5a2e832757945/xgboost-0.90-py2.py3-none-win_amd64.whl (18.3MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\xamir\\anaconda3\\lib\\site-packages (from xgboost) (1.16.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\xamir\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\xamir\\Documents\\UROP19\\DataSets\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\xamir\\Documents\\UROP19\\DataSets\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2358657/2358657 [00:00<00:00, 3519178.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bonds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:08<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting and condensing bonds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2358657/2358657 [00:11<00:00, 199912.12it/s]\n",
      "100%|██████████| 2358657/2358657 [00:14<00:00, 157978.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating hybridization....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2358657/2358657 [01:32<00:00, 25403.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating pi bonds....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2358657/2358657 [01:33<00:00, 25194.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# libraries required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def reduce_memory(df, verbose=True):\n",
    "    \"\"\"\n",
    "    :param: df - dataframe required to decrease the memory usage\n",
    "    :param: verbose - show logging output if 'Ture'\n",
    "\n",
    "    Goal: Reduce the memory usage by decreasing the type of the value if applicable\n",
    "\n",
    "    Return: original dataframe with lower memory usage\n",
    "    \"\"\"\n",
    "\n",
    "    numerics = ['int64', 'int16', 'int32', 'float64', 'float32', 'float16']\n",
    "    start_memory = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_memory = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_memory, 100 * (start_memory - end_memory) / start_memory))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def struc1_merge(df1, df2, index):\n",
    "    \"\"\"\n",
    "    :param: df1 - training data\n",
    "    :param: df2 - structure data after being added electronegativity, radius, bond_lengths, hybridization, surrounding atoms (bonds),\n",
    "            position info. (x, y, z)\n",
    "    :param: index - atom_index in the coupling\n",
    "\n",
    "    Goal: Merge original training dataframe with processed structure data to form a new dataframe for further training process\n",
    "\n",
    "    Return: Merged dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    struc1_train_merge = pd.merge(df1, df2, how='left',\n",
    "                                  left_on=['molecule_name', f'atom_index_{index}', f'atom_{index}', f'x_{index}', f'y_{index}', f'z_{index}'],\n",
    "                                  right_on=['molecule_name', 'atom_index', 'atom', 'x', 'y', 'z'])\n",
    "    \n",
    "    struc1_train_merge = struc1_train_merge.drop(['n_bonds'], axis=1)\n",
    "    \n",
    "    struc1_train_merge = struc1_train_merge.rename(columns={'EN': f'EN_{index}',\n",
    "                                                            'RD': f'RD_{index}',\n",
    "                                                            'bond_lengths': f'bond_lengths_{index}',\n",
    "                                                            'hybri': f'hybri_{index}',\n",
    "                                                            'bonds': f'bonds_{index}',\n",
    "                                                            'pi_bonds': f'pi_bonds_{index}'})\n",
    "    \n",
    "    return struc1_train_merge\n",
    "\n",
    "\n",
    "def n_bonds(structures):\n",
    "    \"\"\"\n",
    "    :param: structures - structure.csv from local data\n",
    "    \n",
    "    Goal: Calculate the number of bonds for each molecule.\n",
    "\n",
    "    Return: Structure dataframe with number of bonds (n_bonds) and lists consisting of indexes of connecting atoms (bonds)\n",
    "    \"\"\"\n",
    "\n",
    "    i_atom = structures['atom_index'].values\n",
    "    p = structures[['x', 'y', 'z']].values\n",
    "    p_compare = p\n",
    "    m = structures['molecule_name'].values\n",
    "    m_compare = m\n",
    "    r = structures['RD'].values\n",
    "    r_compare = r\n",
    "\n",
    "    source_row = np.arange(len(structures))\n",
    "    max_atoms = 28\n",
    "\n",
    "    bonds = np.zeros((len(structures)+1, max_atoms+1), dtype=np.int8)\n",
    "    bond_dists = np.zeros((len(structures)+1, max_atoms+1), dtype=np.float32)\n",
    "\n",
    "    print('Calculating bonds')\n",
    "\n",
    "    for i in tqdm(range(max_atoms-1)):\n",
    "        p_compare = np.roll(p_compare, -1, axis=0)\n",
    "        m_compare = np.roll(m_compare, -1, axis=0)\n",
    "        r_compare = np.roll(r_compare, -1, axis=0)\n",
    "\n",
    "        mask = np.where(m == m_compare, 1, 0) #Are we still comparing atoms in the same molecule?\n",
    "        dists = np.linalg.norm(p - p_compare, axis=1) * mask\n",
    "        r_bond = r + r_compare\n",
    "\n",
    "        bond = np.where(np.logical_and(dists > 0.0001, dists < r_bond), 1, 0)\n",
    "\n",
    "        source_row = source_row\n",
    "        target_row = source_row + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "        target_row = np.where(np.logical_or(target_row > len(structures), mask==0), len(structures), target_row) #If invalid target, write to dummy row\n",
    "\n",
    "        source_atom = i_atom\n",
    "        target_atom = i_atom + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "        target_atom = np.where(np.logical_or(target_atom > max_atoms, mask==0), max_atoms, target_atom) #If invalid target, write to dummy col\n",
    "\n",
    "        bonds[(source_row, target_atom)] = bond\n",
    "        bonds[(target_row, source_atom)] = bond\n",
    "        bond_dists[(source_row, target_atom)] = dists\n",
    "        bond_dists[(target_row, source_atom)] = dists\n",
    "\n",
    "    bonds = np.delete(bonds, axis=0, obj=-1) #Delete dummy row\n",
    "    bonds = np.delete(bonds, axis=1, obj=-1) #Delete dummy col\n",
    "    bond_dists = np.delete(bond_dists, axis=0, obj=-1) #Delete dummy row\n",
    "    bond_dists = np.delete(bond_dists, axis=1, obj=-1) #Delete dummy col\n",
    "\n",
    "    print('Counting and condensing bonds')\n",
    "\n",
    "    bonds_numeric = [[i for i,x in enumerate(row) if x] for row in tqdm(bonds)]\n",
    "    bond_lengths = [[dist for i,dist in enumerate(row) if i in bonds_numeric[j]] for j,row in enumerate(tqdm(bond_dists))]\n",
    "    n_bonds = [len(x) for x in bonds_numeric]\n",
    "\n",
    "    #bond_data = {'bond_' + str(i):col for i, col in enumerate(np.transpose(bonds))}\n",
    "    #bond_data.update({'bonds_numeric':bonds_numeric, 'n_bonds':n_bonds})\n",
    "\n",
    "    bond_data = {'bonds':bonds_numeric, 'n_bonds':n_bonds, 'bond_lengths':bond_lengths}\n",
    "    bond_df = pd.DataFrame(bond_data)\n",
    "    structures = structures.join(bond_df)\n",
    "    \n",
    "    return structures\n",
    "\n",
    "\n",
    "def struc_merge(df, struc, index):\n",
    "    \"\"\"\n",
    "    :param: df - The dataframe to be merged with structure data\n",
    "    :param: struc - structure data\n",
    "    :param: index - index of atom in the coupling\n",
    "\n",
    "    Goal: Merger two dataframe.\n",
    "\n",
    "    Return: a new dataframe after merged\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge train and structures data based on the atom index\n",
    "    df_struc = pd.merge(df, struc, how='left', \n",
    "                        left_on=['molecule_name', f'atom_index_{index}'], \n",
    "                        right_on=['molecule_name', 'atom_index'])\n",
    "\n",
    "    # Drop the atom index column\n",
    "    df_struc = df_struc.drop('atom_index', axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    df_struc = df_struc.rename(columns={'atom': f'atom_{index}',\n",
    "                                        'x': f'x_{index}',\n",
    "                                        'y': f'y_{index}',\n",
    "                                        'z': f'z_{index}'})\n",
    "\n",
    "    return df_struc\n",
    "\n",
    "\n",
    "def distance(df, structures):\n",
    "    \"\"\"\n",
    "    :param: df - Data that need to calculate distance\n",
    "\n",
    "    Goal: Calculate the distance between two spins\n",
    "\n",
    "    Return: DataFrame with distance added\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of  the data for avoiding changing the original data\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Merge data\n",
    "    df_copy = struc_merge(df_copy, structures, 0)\n",
    "    df_copy = struc_merge(df_copy, structures, 1)\n",
    "\n",
    "    # This block for calculating the distance between two spins\n",
    "    df_p_0 = df_copy[['x_0', 'y_0', 'z_0']].values\n",
    "    df_p_1 = df_copy[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "    df_copy['distance'] = np.linalg.norm(df_p_0 - df_p_1, axis=1)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def hybridization(structures):\n",
    "    \"\"\"\n",
    "    :param: structures - structures data\n",
    "\n",
    "    Goal: Calculate each hybridization in the structures data\n",
    "\n",
    "    Return: structure data with hybridization column added\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Calculating hybridization....')\n",
    "    \n",
    "    # 'C' has different types of hybridizations with different number of bonds.\n",
    "    # '4' for four bonds\n",
    "    hybri_dict = {'C': {'4': 3, '3': 2, '2': 2, '1': 0},\n",
    "                  'N': {'4': 0, '3': 3, '2': 2, '1': 1},\n",
    "                  'O': {'2': 2, '1': 1},\n",
    "                  'H': {'1': 0},\n",
    "                  'F': {'1': 0}}\n",
    "                # 3 bonds- sp3, 2 - sp2, 1 - sp\n",
    "    \n",
    "    hybri = []\n",
    "\n",
    "    for i in tqdm(range(len(structures))):\n",
    "        hybri.append(hybri_dict[structures.loc[i, 'atom']][str(structures.loc[i, 'n_bonds'])])\n",
    "    \n",
    "    structures['hybri'] = hybri\n",
    "\n",
    "    return structures\n",
    "\n",
    "\n",
    "def pi_bonds(structures):\n",
    "    \"\"\"\n",
    "    :param: structures - structures data\n",
    "\n",
    "    Goal: Calculate the number of pi_bonds for each atom\n",
    "\n",
    "    Return: structures with pi_bonds column added\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Calculating pi bonds....')\n",
    "    \n",
    "    # The number of atoms connecting to an atom is related with the number of pi bonds.\n",
    "    # Eg: In 'C', if there are 4 bonds around, then the number of pi bonds is 0.\n",
    "    pi_bond = {'C': {'4': 0, '2': 2, '3': 1},\n",
    "               'N': {'4': 0, '3': 0, '2': 1, '1': 2},\n",
    "               'O': {'1': 1, '2': 0},\n",
    "               'H': {'1': 0},\n",
    "               'F': {'1': 0}}\n",
    "\n",
    "    pi_bond_ = []\n",
    "\n",
    "    for i in tqdm(range(len(structures))):\n",
    "        pi_bond_.append(pi_bond[structures.loc[i, 'atom']][str(structures.loc[i, 'n_bonds'])])\n",
    "\n",
    "    structures['pi_bonds'] = pi_bond_\n",
    "\n",
    "    return structures\n",
    "\n",
    "\n",
    "def electronegativity(atom_name, structures):\n",
    "    \"\"\"\n",
    "    :param: atom_name - list or np.ndarray consisting of name of atoms\n",
    "    :param: structures - structures data\n",
    "\n",
    "    Goal: Assign an electrinegativity for each atom\n",
    "\n",
    "    Return: structures with electrineativity column added\n",
    "    \"\"\"\n",
    "    \n",
    "    electronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\n",
    "    en_ = [electronegativity[x] for x in tqdm(atom_name)]\n",
    "\n",
    "    structures['EN'] = en_\n",
    "\n",
    "    return structures\n",
    "\n",
    "\n",
    "def radius(atom_name, structures):\n",
    "    \"\"\"\n",
    "    :param: atom_name - list or np.ndarray consisting of name of atoms\n",
    "    :param: structures - structures data\n",
    "\n",
    "    Goal: Assign an radius for each atom\n",
    "\n",
    "    Return: structures with radius column added\n",
    "    \"\"\"\n",
    "    \n",
    "    atomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n",
    "\n",
    "    fudge_factor = 0.05\n",
    "    atomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\n",
    "    rd_ = [atomic_radius[x] for x in atom_name]\n",
    "\n",
    "    structures['RD'] = rd_\n",
    "\n",
    "    return structures\n",
    "\n",
    "\n",
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    \"\"\"\n",
    "    :param: df_1 - train data\n",
    "    :param: df_2 - structure data\n",
    "    :param: atom_ind - atom index in coupling\n",
    "\n",
    "    Goal: Merge two dataframe for further using\n",
    "\n",
    "    Return: A new dataframe after merged\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_closest(df_train):\n",
    "    df_temp=df_train.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"distance\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_=df_temp.copy()\n",
    "    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "\n",
    "    df_temp=pd.concat(objs=[df_temp,df_temp_],axis=0)\n",
    "\n",
    "    df_temp[\"min_distance\"]=df_temp.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('min')\n",
    "    df_temp= df_temp[df_temp[\"min_distance\"]==df_temp[\"distance\"]]\n",
    "\n",
    "    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                     'atom_index_1': 'atom_index_closest',\n",
    "                                     'distance': 'distance_closest',\n",
    "                                     'x_1': 'x_closest',\n",
    "                                     'y_1': 'y_closest',\n",
    "                                     'z_1': 'z_closest'})\n",
    "\n",
    "    for atom_idx in [0,1]:\n",
    "        df_train = map_atom_info(df_train,df_temp, atom_idx)\n",
    "        df_train = df_train.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                            'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                            'x_closest': f'x_closest_{atom_idx}',\n",
    "                                            'y_closest': f'y_closest_{atom_idx}',\n",
    "                                            'z_closest': f'z_closest_{atom_idx}'})\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def add_cos_features(df):\n",
    "    \"\"\"\n",
    "    :param: df - dataframe containing necessary data for calculating the cosine value\n",
    "\n",
    "    Goal: Calculating cosine value\n",
    "\n",
    "    Return: dataframe with cosine data added\n",
    "    \"\"\"\n",
    "\n",
    "    # The modulus of the \n",
    "    df[\"distance_0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"distance_1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "    \n",
    "    # Unit vector along each direction\n",
    "    df[\"vec_0_x\"]=(df['x_0']-df['x_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_y\"]=(df['y_0']-df['y_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_z\"]=(df['z_0']-df['z_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_1_x\"]=(df['x_1']-df['x_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_y\"]=(df['y_1']-df['y_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_z\"]=(df['z_1']-df['z_closest_1'])/df[\"distance_1\"]\n",
    "    \n",
    "    # Ratio between the difference along each direction to the distance\n",
    "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"distance\"]\n",
    "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"distance\"]\n",
    "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"distance\"]\n",
    "\n",
    "    # Cosine of each component\n",
    "    df[\"cos_0_1\"]=df[\"vec_0_x\"]*df[\"vec_1_x\"]+df[\"vec_0_y\"]*df[\"vec_1_y\"]+df[\"vec_0_z\"]*df[\"vec_1_z\"]\n",
    "    df[\"cos_0\"]=df[\"vec_0_x\"]*df[\"vec_x\"]+df[\"vec_0_y\"]*df[\"vec_y\"]+df[\"vec_0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_1\"]=df[\"vec_1_x\"]*df[\"vec_x\"]+df[\"vec_1_y\"]*df[\"vec_y\"]+df[\"vec_1_z\"]*df[\"vec_z\"]\n",
    "\n",
    "    df=df.drop(['vec_0_x','vec_0_y','vec_0_z','vec_1_x','vec_1_y','vec_1_z','vec_x','vec_y','vec_z'], axis=1)\n",
    "\n",
    "    # Angle for each component\n",
    "    df[\"Angle\"] = df[\"cos_0_1\"].apply(lambda x: np.arccos(x)) * 180 / np.pi\n",
    "    df[\"cos_0\"] = df[\"cos_0\"].apply(lambda x: np.arccos(x)) * 180 / np.pi\n",
    "    df[\"cos_1\"] = df[\"cos_1\"].apply(lambda x: np.arccos(x)) * 180 / np.pi\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# File paths\n",
    "train_path = r\"C:\\Users\\xamir\\Documents\\UROP19\\DataSets\\train.csv\"\n",
    "structures_path = r\"C:\\Users\\xamir\\Documents\\UROP19\\DataSets\\structures.csv\"\n",
    "test_path = r\"C:\\Users\\xamir\\Documents\\UROP19\\DataSets\\test.csv\"\n",
    "\n",
    "# read data from local address\n",
    "train_df_full = pd.read_csv(train_path, index_col=0)\n",
    "structures_df_full = pd.read_csv(structures_path, dtype={'atom_index': np.int8})\n",
    "test_df_full = pd.read_csv(test_path)\n",
    "\n",
    "# Add distance feature to the test and trin data\n",
    "train_df = distance(train_df_full, structures_df_full)\n",
    "test_df = distance(test_df_full, structures_df_full)\n",
    "\n",
    "# ndarray with names of each atom in the structures csv\n",
    "atom = structures_df_full['atom'].values\n",
    "\n",
    "# Add electronegativity and radius colmun to the structures csv\n",
    "structures = electronegativity(atom, structures_df_full)\n",
    "structures = radius(atom, structures)\n",
    "\n",
    "# Add number of bonds and connecting atoms columns\n",
    "structures = n_bonds(structures)\n",
    "\n",
    "# Add hybridization column\n",
    "structures = hybridization(structures)\n",
    "\n",
    "# Add pi_bonds column\n",
    "structures = pi_bonds(structures)\n",
    "\n",
    "# Merge structures data and train data\n",
    "struc_train = struc1_merge(train_df, structures, 0)\n",
    "struc_train = struc1_merge(struc_train, structures, 1)\n",
    "\n",
    "struc_train = struc_train.drop(['atom_index_x', 'atom_x', 'x_x', 'y_x', 'z_x',\n",
    "                                'atom_index_y', 'atom_y','x_y', 'y_y', 'z_y'], axis=1)\n",
    "\n",
    "# Add bond angle column\n",
    "struc_train = create_closest(struc_train)\n",
    "struc_train = add_cos_features(struc_train)\n",
    "\n",
    "# The list of type for further training\n",
    "type_list = list(struc_train['type'].unique())\n",
    "\n",
    "# Drop the target column for training\n",
    "y = struc_train['scalar_coupling_constant']\n",
    "struc_train = struc_train.drop(['scalar_coupling_constant'], axis=1)\n",
    "\n",
    "# Select features for training\n",
    "X = struc_train[['molecule_name',\n",
    "                           'type',\n",
    "                           'distance',\n",
    "                           'EN_0',\n",
    "                           'RD_0',\n",
    "                           'hybri_0',\n",
    "                           'pi_bonds_0',\n",
    "                           'EN_1',\n",
    "                           'RD_1',\n",
    "                           'hybri_1',\n",
    "                           'pi_bonds_1',\n",
    "                           'Angle']]\n",
    "\n",
    "struc_test = struc1_merge(test_df, structures, 0)\n",
    "struc_test = struc1_merge(struc_test, structures, 1)\n",
    "\n",
    "struc_test = struc_test.drop(['atom_index_x', 'atom_x', 'x_x', 'y_x', 'z_x',\n",
    "                                'atom_index_y', 'atom_y','x_y', 'y_y', 'z_y'], axis=1)\n",
    "\n",
    "\n",
    "struc_test = create_closest(struc_test)\n",
    "struc_test = add_cos_features(struc_test)\n",
    "\n",
    "# The list of type for further training\n",
    "type_list = list(struc_test['type'].unique())\n",
    "\n",
    "\n",
    "# Select features for training\n",
    "X_test = struc_test[['molecule_name',\n",
    "                           'type',\n",
    "                           'distance',\n",
    "                           'EN_0',\n",
    "                           'RD_0',\n",
    "                           'hybri_0',\n",
    "                           'pi_bonds_0',\n",
    "                           'EN_1',\n",
    "                           'RD_1',\n",
    "                           'hybri_1',\n",
    "                           'pi_bonds_1',\n",
    "                           'Angle']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>type</th>\n",
       "      <th>distance</th>\n",
       "      <th>EN_0</th>\n",
       "      <th>RD_0</th>\n",
       "      <th>hybri_0</th>\n",
       "      <th>pi_bonds_0</th>\n",
       "      <th>EN_1</th>\n",
       "      <th>RD_1</th>\n",
       "      <th>hybri_1</th>\n",
       "      <th>pi_bonds_1</th>\n",
       "      <th>Angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>1.091953059611900</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>70.528681978087036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>1.783119756038801</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.468408167182687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>1.783147496403011</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.471318021912978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>1.783156685329616</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.472066914238823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>1.091951618581363</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>70.527651358140233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  type           distance  EN_0  RD_0  hybri_0  pi_bonds_0  \\\n",
       "0  dsgdb9nsd_000001  1JHC  1.091953059611900   2.2  0.43        0           0   \n",
       "1  dsgdb9nsd_000001  2JHH  1.783119756038801   2.2  0.43        0           0   \n",
       "2  dsgdb9nsd_000001  2JHH  1.783147496403011   2.2  0.43        0           0   \n",
       "3  dsgdb9nsd_000001  2JHH  1.783156685329616   2.2  0.43        0           0   \n",
       "4  dsgdb9nsd_000001  1JHC  1.091951618581363   2.2  0.43        0           0   \n",
       "\n",
       "   EN_1  RD_1  hybri_1  pi_bonds_1                Angle  \n",
       "0  2.55  0.82        3           0   70.528681978087036  \n",
       "1  2.20  0.43        0           0  109.468408167182687  \n",
       "2  2.20  0.43        0           0  109.471318021912978  \n",
       "3  2.20  0.43        0           0  109.472066914238823  \n",
       "4  2.55  0.82        3           0   70.527651358140233  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>type</th>\n",
       "      <th>distance</th>\n",
       "      <th>EN_0</th>\n",
       "      <th>RD_0</th>\n",
       "      <th>hybri_0</th>\n",
       "      <th>pi_bonds_0</th>\n",
       "      <th>EN_1</th>\n",
       "      <th>RD_1</th>\n",
       "      <th>hybri_1</th>\n",
       "      <th>pi_bonds_1</th>\n",
       "      <th>Angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>2.2611780778</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>1.0620990942</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3JHH</td>\n",
       "      <td>3.3232771720</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>1.0620990942</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2JHC</td>\n",
       "      <td>2.2611780778</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  type      distance  EN_0  RD_0  hybri_0  pi_bonds_0  \\\n",
       "0  dsgdb9nsd_000004  2JHC  2.2611780778   2.2  0.43        0           0   \n",
       "1  dsgdb9nsd_000004  1JHC  1.0620990942   2.2  0.43        0           0   \n",
       "2  dsgdb9nsd_000004  3JHH  3.3232771720   2.2  0.43        0           0   \n",
       "3  dsgdb9nsd_000004  1JHC  1.0620990942   2.2  0.43        0           0   \n",
       "4  dsgdb9nsd_000004  2JHC  2.2611780778   2.2  0.43        0           0   \n",
       "\n",
       "   EN_1  RD_1  hybri_1  pi_bonds_1  Angle  \n",
       "0  2.55  0.82        2           2    0.0  \n",
       "1  2.55  0.82        2           2  180.0  \n",
       "2  2.20  0.43        0           0  180.0  \n",
       "3  2.55  0.82        2           2  180.0  \n",
       "4  2.55  0.82        2           2    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([\"molecule_name\", \"type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop([\"molecule_name\", \"type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-25-2dd696f68327>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-2dd696f68327>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    result_dict = {}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "        result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 50,\n",
    "          'min_child_samples': 79,\n",
    "          'min_data_in_leaf' : 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    84.807599999999994\n",
       "1   -11.257000000000000\n",
       "2   -11.254799999999999\n",
       "3   -11.254300000000001\n",
       "4    84.807400000000001\n",
       "Name: scalar_coupling_constant, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Aug  4 15:37:04 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.98878\tvalid_1's l1: 2.0495\n",
      "[1000]\ttraining's l1: 1.9779\tvalid_1's l1: 2.03588\n",
      "[1500]\ttraining's l1: 1.97299\tvalid_1's l1: 2.0299\n",
      "[2000]\ttraining's l1: 1.97016\tvalid_1's l1: 2.02697\n",
      "[2500]\ttraining's l1: 1.96815\tvalid_1's l1: 2.02499\n",
      "[3000]\ttraining's l1: 1.96649\tvalid_1's l1: 2.02312\n",
      "[3500]\ttraining's l1: 1.96543\tvalid_1's l1: 2.02204\n",
      "[4000]\ttraining's l1: 1.96433\tvalid_1's l1: 2.02087\n",
      "[4500]\ttraining's l1: 1.9635\tvalid_1's l1: 2.02018\n",
      "[5000]\ttraining's l1: 1.96268\tvalid_1's l1: 2.01933\n",
      "[5500]\ttraining's l1: 1.96197\tvalid_1's l1: 2.01858\n",
      "[6000]\ttraining's l1: 1.9613\tvalid_1's l1: 2.01779\n",
      "[6500]\ttraining's l1: 1.96086\tvalid_1's l1: 2.01731\n",
      "[7000]\ttraining's l1: 1.96045\tvalid_1's l1: 2.01693\n",
      "[7500]\ttraining's l1: 1.95996\tvalid_1's l1: 2.01651\n",
      "[8000]\ttraining's l1: 1.95943\tvalid_1's l1: 2.01608\n",
      "Early stopping, best iteration is:\n",
      "[8258]\ttraining's l1: 1.95908\tvalid_1's l1: 2.01557\n",
      "Fold 2 started at Sun Aug  4 16:18:42 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.93221\tvalid_1's l1: 2.22955\n",
      "[1000]\ttraining's l1: 1.91966\tvalid_1's l1: 2.21932\n",
      "[1500]\ttraining's l1: 1.91424\tvalid_1's l1: 2.21536\n",
      "[2000]\ttraining's l1: 1.91139\tvalid_1's l1: 2.21356\n",
      "[2500]\ttraining's l1: 1.90913\tvalid_1's l1: 2.21208\n",
      "[3000]\ttraining's l1: 1.90759\tvalid_1's l1: 2.21115\n",
      "[3500]\ttraining's l1: 1.90613\tvalid_1's l1: 2.21022\n",
      "[4000]\ttraining's l1: 1.9049\tvalid_1's l1: 2.20936\n",
      "[4500]\ttraining's l1: 1.90383\tvalid_1's l1: 2.20884\n",
      "[5000]\ttraining's l1: 1.90293\tvalid_1's l1: 2.20828\n",
      "Early stopping, best iteration is:\n",
      "[4962]\ttraining's l1: 1.90289\tvalid_1's l1: 2.20814\n",
      "Fold 3 started at Sun Aug  4 16:40:55 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.97891\tvalid_1's l1: 2.07674\n",
      "[1000]\ttraining's l1: 1.9659\tvalid_1's l1: 2.07044\n",
      "[1500]\ttraining's l1: 1.96053\tvalid_1's l1: 2.06804\n",
      "[2000]\ttraining's l1: 1.95709\tvalid_1's l1: 2.06647\n",
      "[2500]\ttraining's l1: 1.95428\tvalid_1's l1: 2.06545\n",
      "[3000]\ttraining's l1: 1.95216\tvalid_1's l1: 2.06451\n",
      "[3500]\ttraining's l1: 1.95039\tvalid_1's l1: 2.06389\n",
      "[4000]\ttraining's l1: 1.94925\tvalid_1's l1: 2.06337\n",
      "[4500]\ttraining's l1: 1.94845\tvalid_1's l1: 2.06306\n",
      "Early stopping, best iteration is:\n",
      "[4539]\ttraining's l1: 1.9483\tvalid_1's l1: 2.06297\n",
      "Fold 4 started at Sun Aug  4 17:00:26 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 2.01237\tvalid_1's l1: 1.95\n",
      "[1000]\ttraining's l1: 2.00018\tvalid_1's l1: 1.94373\n",
      "[1500]\ttraining's l1: 1.99429\tvalid_1's l1: 1.94035\n",
      "[2000]\ttraining's l1: 1.99075\tvalid_1's l1: 1.93883\n",
      "[2500]\ttraining's l1: 1.98848\tvalid_1's l1: 1.93794\n",
      "[3000]\ttraining's l1: 1.9865\tvalid_1's l1: 1.93698\n",
      "[3500]\ttraining's l1: 1.9851\tvalid_1's l1: 1.93634\n",
      "[4000]\ttraining's l1: 1.98394\tvalid_1's l1: 1.93593\n",
      "[4500]\ttraining's l1: 1.98299\tvalid_1's l1: 1.93563\n",
      "[5000]\ttraining's l1: 1.98209\tvalid_1's l1: 1.93528\n",
      "Early stopping, best iteration is:\n",
      "[4879]\ttraining's l1: 1.9821\tvalid_1's l1: 1.93506\n",
      "Fold 5 started at Sun Aug  4 17:21:53 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 2.02863\tvalid_1's l1: 1.89209\n",
      "[1000]\ttraining's l1: 2.01612\tvalid_1's l1: 1.88048\n",
      "[1500]\ttraining's l1: 2.01059\tvalid_1's l1: 1.87558\n",
      "[2000]\ttraining's l1: 2.00769\tvalid_1's l1: 1.87353\n",
      "[2500]\ttraining's l1: 2.00556\tvalid_1's l1: 1.87219\n",
      "[3000]\ttraining's l1: 2.00426\tvalid_1's l1: 1.87164\n",
      "[3500]\ttraining's l1: 2.00304\tvalid_1's l1: 1.87083\n",
      "[4000]\ttraining's l1: 2.00176\tvalid_1's l1: 1.8698\n",
      "[4500]\ttraining's l1: 2.00093\tvalid_1's l1: 1.86923\n",
      "[5000]\ttraining's l1: 2.00001\tvalid_1's l1: 1.86851\n",
      "[5500]\ttraining's l1: 1.9993\tvalid_1's l1: 1.86783\n",
      "[6000]\ttraining's l1: 1.99876\tvalid_1's l1: 1.86747\n",
      "[6500]\ttraining's l1: 1.99816\tvalid_1's l1: 1.86715\n",
      "[7000]\ttraining's l1: 1.99755\tvalid_1's l1: 1.86656\n",
      "[7500]\ttraining's l1: 1.997\tvalid_1's l1: 1.86631\n",
      "Early stopping, best iteration is:\n",
      "[7636]\ttraining's l1: 1.99686\tvalid_1's l1: 1.86615\n",
      "CV mean score: 2.0176, std: 0.1167.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'oof': array([ 89.07343006, -10.37144516, -10.37144516, ...,   2.69567762,\n",
       "          2.42718641, 123.77860968]),\n",
       " 'prediction': array([ 11.45771247, 196.160502  ,   6.22213519, ...,   4.01569921,\n",
       "          3.05420496, 122.81882823]),\n",
       " 'scores': [2.0155685803611187,\n",
       "  2.208135206984266,\n",
       "  2.0629710168716135,\n",
       "  1.9350599810733726,\n",
       "  1.8661452631891104]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_regression(X, X_test, y, params=params, folds = folds, model_type = 'lgb', eval_metric = 'mae', plot_feature_importance=False,\n",
    "                      verbose = 500, early_stopping_rounds=200, n_estimators = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_short_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c5ad5c74859f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scalar_coupling_constant'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_short_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission_type.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_short_test' is not defined"
     ]
    }
   ],
   "source": [
    "sub['scalar_coupling_constant'] = X_short_test['prediction']\n",
    "sub.to_csv('submission_type.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
